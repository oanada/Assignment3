---
title: "CourseProject-GroupA"
author: 'Group A - Autumn Class:  Junylou Daniel, Oana Damian, Robin Mathew, Torsten Meyer'
date: "November 2020"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE,
	out.width="100%",
	fig.fullwidth=TRUE, 
	fig.align='center',
	fig.asp = .6
)
```



# 1. Business case

### 1.1 Business objectives

1.  What is the market value of my house?
3.  With a given budget and other constraints, where is the best location I can buy a house?
 
 
### 1.2 Assessment

The pertinent data will be gathered from different sources.
1.  https://www.zoocasa.com - Available history of sold houses/condos  
2.  https://open.toronto.ca/dataset/neighbourhoods/ - Toronto neighbourhood boundary data
3.  https://torontolife.com/neighbourhood-rankings/ - Website that ranks Toronto neighbourhood in terms of Housing, Safety, Transit, etc.


Software Required:
1.  R Library
	
Hardware Required:
1.  

Assumption

Constraints
	
Terminology

Cost and Benefits

### 1.3 Project Goals:
1.  Predict house price given the location, type and features of the house.
2.  Identify what features of the house mostly contribute to an increased value of the house
3.  Identify which location has the best value given the type of house, features and neighbourhood. 

The project will use clustering and linear prediction algorithm.  We expect an acceptable accuracy of 90%.


### 1.4 Project plan
???




# 2. Data understanding


### 2.1 Data sources

The current project builds upon an existing project on Toronto housing price prediction available at <https://github.com/slavaspirin/Toronto-housing-price-prediction>
The data used in the current project is available at <https://github.com/slavaspirin/Toronto-housing-price-prediction/raw/master/houses_edited.csv>
The data is based on web scarping of Zoocasa listings of previously sold properties.
Unfortunately the data does not have a time stamp. We understand that the web scraping was undertaken sometimes at the end of 2019.

The primary listing data was scraped from <https://www.zoocasa.com> and contains a list sold houses/condos available as sometimes in H2 2019.


The ranking of Toronto neighbourhoods  was scraped from <https://torontolife.com/neighbourhood-rankings/>.The ranking is in terms of Housing, Safety, Transit, etc.
 

Toronto neighborhoods data for geographical mapping is available from <https://open.toronto.ca/dataset/neighbourhoods/> - 



### 2.2 Data description


#### Zoocasa listings

The data available includes 15234 listings of Toronto houses and condos with the following available features.

Variable Name |   Description
--------------|---------------------------------------------
title         | Zoocasa short description of the listing
final price   | sale price
listed price  | listed price
bedrooms      | 0 beds, 0 + 1 beds, 1 beds  ... 9 + 5 beds
bathrooms     | 1 to 11
sqft          | Missing or numeric between 259 to 4374 
description   | Zoocasa long description of the listed property
mls           | zoocasa identifier
type          | Att/Row/Twnhouse, Comm Element Condo, Condo Apt, Condo Townhouse, Co-Op Apt, Co-Ownership Apt, Detached, Link, Plex, Semi-Detached, Store W/Apt/Offc
full link     | Zoocasa web link
lat           | property location latitude
long          | property location longitude
city district | Toronto city district
district code | Toronto city district identifier code
mean district income| Toronto city district average household income based on 2016 statics


#### Toronto neighbourhood rankings

Variable Name |   Description
--------------|---------------------------------------------
district code | Toronto city district identifier code
area name     | Toronto city district name
description   | description of the listing
housing rank  | website ranking of housing conditions
housing rank  | website ranking of housing conditions
safety rank   | website ranking of safety conditions
transit rank  | website ranking of housing conditions
housing rank  | website ranking of transit conditions
shopping rank | website ranking of shopping conditions
health rank   | website ranking of healthcare conditions
entertainment rank | website ranking of entertainment venues
community rank | website ranking of community conditions
education rank | website ranking of education conditions
diversity rank | website ranking of diversity status
employment rank | website ranking of employment conditions



```{r, message = FALSE, echo = FALSE, out.width="100%",fig.fullwidth=TRUE, fig.align='center',fig.asp = .6}
# libraries
# library(naniar)
# library(RColorBrewer)
library(dplyr)
library(readr)
# library(caTools)
# library(corrplot)
# library(ggpubr)
library(ggplot2)
library(gridExtra)
# library(cluster)
# library(fpc)
# library(purrr)
# library(dendextend)

library(data.table)
library(leaflet)
library(maps)
library(rgdal)

library(MASS)
library(stats)
library(mice)

### clear workspace
rm(list = ls(all.names = TRUE)) #will clear all objects includes hidden objects.
memory.limit(size=50000)  #gc(verbose=FALSE) #free up memory and report the memory usage.

# seed for all random number generators
RandomNbGenSeed = 5000
set.seed(RandomNbGenSeed)


```





### 2.3 Descriptive statistics


```{r, message = FALSE, warning=FALSE, echo = FALSE, out.width="100%",fig.fullwidth=TRUE, fig.align='center',fig.asp = .6}

### ==============================================================
### read house listings data files
### ==============================================================


# housing listing data
INPUT_file_path="https://github.com/slavaspirin/Toronto-housing-price-prediction/raw/master/houses_edited.csv"
DF<-read.csv(INPUT_file_path, header = TRUE)

DF$mean_district_income_bins<-case_when( #unique((quantile(DF$mean_district_income, probs = seq(0,1, length.out = 10),na.rm = TRUE)/10)*10)
  DF$mean_district_income <=31500 ~ "( 0   - 31.5K]",
  DF$mean_district_income <=34000 ~ "(31.5 - 34.0K]",
  DF$mean_district_income <=37500 ~ "(34.0 - 37.5K]",
  DF$mean_district_income <=45000 ~ "(37.5 - 45.0K]",
  DF$mean_district_income <=50500 ~ "(45.0 - 50.5K]",
  DF$mean_district_income <=54000 ~ "(50.5 - 54.0K]",
  DF$mean_district_income <=57500 ~ "(54.0 - 57.5K]",
  DF$mean_district_income <=70000 ~ "(57.5 - 70.0K]",
  DF$mean_district_income <=80000 ~ "(70.0 - 80.0K]",
  DF$mean_district_income >80000 ~  "(80.0K+",
)

DF$sqft_bins<-case_when(    #unique((quantile(DF$sqft, probs = seq(0,1, length.out = 10),na.rm = TRUE)/10)*10)
  DF$sqft <=550  ~ "( 250 - 550]",
  DF$sqft <=650  ~ "( 550 - 650]",
  DF$sqft <=750  ~ "( 650 - 750]",
  DF$sqft <=850  ~ "( 750 - 850]",
  DF$sqft <=950  ~ "( 850 - 950]",
  DF$sqft <=1100 ~ "( 950 - 1100]",
  DF$sqft <=1300 ~ "(1100 - 1300]",
  DF$sqft <=1800 ~ "(1300 - 1800]",
  DF$sqft >1800 ~  "(1800+",
)

DF$sqft_bins<-as.factor(DF$sqft_bins)

DF$type<-as.factor(DF$type)

DF$bathrooms<-as.factor(DF$bathrooms)

DF$bedrooms_all<-as.factor(DF$bedrooms_ag+DF$bedrooms_bg)

DF$pool_available<-as.factor(grepl("Pool|pool", DF$description))
  

### ==============================================================
### read & add add TO district rankings data
### ==============================================================

INPUT_file_path="https://github.com/oanada/Assignment3/raw/main/TO_Neighbourhood.csv"
DF_TO_ranking<-read.csv(INPUT_file_path, header = TRUE)
INPUT_retained_fields<-c('district_code','safety','transit','shopping','health','entertainment','community','diversity','education','employment')
INPUT_retained_fields_rename<-c('district_code','TO_rank_safety','TO_rank_transit','TO_rank_shopping','TO_rank_health','TO_rank_entertainment','TO_rank_community','TO_rank_diversity','TO_rank_education','TO_rank_employment')
DF_TO_ranking<-DF_TO_ranking[,INPUT_retained_fields]
names(DF_TO_ranking)<-INPUT_retained_fields_rename
DF<-merge(x=DF,y=DF_TO_ranking,by="district_code",all.x=TRUE)




### ==============================================================
### read & add subway proximity data
### ==============================================================

INPUT_file_path="https://github.com/oanada/Assignment3/raw/main/GPS_locations_subway_TO.csv"
DF_TO_subways<-read.csv(INPUT_file_path, header = TRUE)


DF$dist_2_subway_km <- NA
DF$dist_2_subway_min <- NA
DF$closest_subway_station <- NA
DF$closest_subway_line <- NA

for (idx_row in seq(length(DF$lat))) 
{ DF_TO_subways$lat_dist_2_location <-DF_TO_subways$Latitude-DF$lat[idx_row]
  DF_TO_subways$long_dist_2_location <- DF_TO_subways$Longitude-DF$long[idx_row]
  DF_TO_subways$dist_2_location_km<- (DF_TO_subways$lat_dist_2_location^2+DF_TO_subways$long_dist_2_location^2)^0.5*10001/90 
  DF_TO_subways <- DF_TO_subways[order(DF_TO_subways$dist_2_location_km),] 

  
  DF$dist_2_subway_km[idx_row] <- DF_TO_subways$dist_2_location_km[1]
  DF$closest_subway_station[idx_row] <- DF_TO_subways$Station_Name[1]
  DF$closest_subway_line[idx_row] <- DF_TO_subways$Line[1]
    
}
DF$dist_2_subway_min<-round(DF$dist_2_subway_km*60/5)
DF$dist_2_subway_km<-round(DF$dist_2_subway_km,3)


DF$dist_2_subway_min_bins_walk<-case_when(
  DF$dist_2_subway_min <=5   ~  "( 0-5min]",
  DF$dist_2_subway_min <=10  ~ "( 5 - 10min]",
  DF$dist_2_subway_min <=15  ~ "(10 - 15min]",
  DF$dist_2_subway_min <=25  ~ "(15 - 25min]",
  DF$dist_2_subway_min <=40  ~ "(25 - 40min]",
  DF$dist_2_subway_min >40~    "(40min+ - ",
)

DF$dist_2_subway_min_bins_10min<-as.factor(round(DF$dist_2_subway_min/10)*10)

INPUT_local_folder="C:/Users_Folders/YORK_MLcertificate/Assignment3/"
write.csv(DF, paste0(INPUT_local_folder,"TO_housing_comprehensive_data.csv"))


### ==============================================================
###   read Toronto map data file
### ==============================================================


# Source: https://open.toronto.ca/dataset/neighbourhoods/
to_neigh <- rgdal::readOGR("https://github.com/oanada/Assignment3/raw/main/Neighbourhoods.geojson")


# add number of district listings to_neigh file
DF_districts=DF[,c("district_code")]
DF_districts=aggregate(x=DF$district_code, by=list(DF$district_code), FUN="length")
names(DF_districts)=c("AREA_SHORT_CODE","count_listings")
to_neigh@data=merge(x=to_neigh@data,y=DF_districts,by="AREA_SHORT_CODE",all.x=TRUE)


# add mean district income to to_neigh file
DF_districts=unique(DF[,c("mean_district_income","district_code")])
DF_districts$AREA_SHORT_CODE=DF_districts$district_code
to_neigh@data=merge(x=to_neigh@data,y=DF_districts[,c("mean_district_income","AREA_SHORT_CODE")],by="AREA_SHORT_CODE",all.x=TRUE)


# add median 3 bedroom house price to to_neigh file
DF_districts=DF[DF$bedrooms %in%  c('2 + 1 beds','3 beds') & DF$type  %in% c("Detached",	"Semi-Detached")
                ,c("final_price","district_code")]
DF_districts=aggregate(x=DF_districts$final_price, by=list(DF_districts$district_code), FUN="mean")
names(DF_districts)=c("AREA_SHORT_CODE","price_3brd_house")
to_neigh@data=merge(x=to_neigh@data,y=DF_districts,by="AREA_SHORT_CODE",all.x=TRUE)


# add median 3 bedroom condo like  price o to_neigh file
DF_districts=DF[DF$bedrooms %in%  c('2 + 1 beds','3 beds') & !(DF$type  %in% c("Detached",	"Semi-Detached"))
                ,c("final_price","district_code")]
DF_districts=aggregate(x=DF_districts$final_price, by=list(DF_districts$district_code), FUN="mean")
names(DF_districts)=c("AREA_SHORT_CODE","price_3brd_condo")
to_neigh@data=merge(x=to_neigh@data,y=DF_districts,by="AREA_SHORT_CODE",all.x=TRUE)



```






##### Final price descriptive statistics by various features

```{r, message = FALSE, echo = FALSE,out.width="100%",fig.fullwidth=TRUE, fig.align='center',fig.asp =0.5}

### ==============================================================
# price descriptive statistics by various features
### ==============================================================

INPUT_vars_boxplots<- c( # Var 1 # var2    # var1 nice name    # var2 nice name   # comments
"type",                      "final_price_log", "House type" ,             "Log sale price"   ,
"As expected, there is a clear dependency between the type of house and the sale price",

"bedrooms_all",              "final_price_log", "Bedrooms All" ,           "Log sale price"   ,
"As expected, there is a clear increasing relationship between the number of bedrooms and the sale price",

"bathrooms",                 "final_price_log", "Bathrooms"     ,          "Log sale price"   ,
"As expected, there is a clear increasing relationship between the number of bathrooms and the sale price",

"sqft_bins",                "final_price_log", "Avg. district income" ,   "Log sale price"   ,
"There is a clear increasing relationship between the surface of property and the sale price. One can observe that there is a lot of missing data which will need to be imputed to fill in missing data.",

"dist_2_subway_min_bins_walk",  "final_price_log", "Walking distance to subway (min)" ,   "Log sale price"   ,
"There is a not a clearrelationship between the distance to the closest subway and the sale price.",

"mean_district_income_bins", "final_price_log", "Avg. district income" ,   "Log sale price"   ,
"There is a clear increasing relationship between the neignourhood wealth level and the sale price" 
)  



INPUT_vars_boxplots<-matrix(INPUT_vars_boxplots,ncol=5,byrow=TRUE)

for(idx_var in 1:nrow(INPUT_vars_boxplots))
  
{ print(INPUT_vars_boxplots[idx_var,5])
  
  DF$VAR_TGT_1<-DF[,INPUT_vars_boxplots[idx_var,1]]
  DF$VAR_TGT_2<-DF[,INPUT_vars_boxplots[idx_var,2]]
  
 p1<-ggplot(DF,aes(x=VAR_TGT_1, y=VAR_TGT_2, fill=VAR_TGT_1)) + 
    geom_violin(width=1.2,show.legend = FALSE,linetype = 0) +
    geom_boxplot(width=.1,show.legend = FALSE) +
    labs(y=INPUT_vars_boxplots[idx_var,4], x=INPUT_vars_boxplots[idx_var,3], 
        title=paste0(INPUT_vars_boxplots[idx_var,4], " distribution by ",INPUT_vars_boxplots[idx_var,3]))+
    theme_bw()
 
 
 p1<-p1+theme(axis.text.x=element_text(angle=60,size=7,vjust =0.5),axis.text=element_text(size=7),
          plot.title = element_text(size=9),legend.position="none")


 p2<- ggplot(DF, aes(x=VAR_TGT_1,fill=VAR_TGT_1) ) + 
      geom_bar(show.legend = FALSE)+
      labs( y="No listings",x=INPUT_vars_boxplots[idx_var,3], 
            title=paste0("No listings by ",INPUT_vars_boxplots[idx_var,3]))+ 
      theme_bw()
 
 p2<-  p2+ theme(axis.text.x=element_text(angle=60,size=7,vjust = 0.5), axis.text=element_text(size=7),
            plot.title = element_text(size=9),legend.position="none")
 
#suppressWarnings(plot(p1))
#plot(p2)
suppressWarnings(grid.arrange(p1,p2,ncol=2))

}  

```



##### Descriptive statistics by geographical district

```{r, message = FALSE,warning=FALSE, echo = FALSE,out.width="100%",fig.fullwidth=TRUE, fig.align='center',fig.asp = 0.5}

#    number of listings by district
### ==============================================================

pal <- colorBin("viridis",domain=c(min(to_neigh$count_listings,na.rm = TRUE),max(to_neigh$count_listings,na.rm = TRUE)),
                bins=round(quantile(to_neigh$count_listings, probs = seq(0,1, length.out = 10),na.rm = TRUE)/5)*5)

labels <- sprintf("<strong>%s</strong>", to_neigh$AREA_NAME) %>% lapply(htmltools::HTML)

to_map <- leaflet(to_neigh) %>% setView(lng = -79.378, lat = 43.695, zoom = 10.5 ) %>% addTiles() %>%
          addMarkers(lng = -79.3841, lat = 43.6534, popup="Toronto City Hall")
to_map  %>% addPolygons(fillColor = ~pal(count_listings), weight = 2, opacity = .3, color = "blue",
              dashArray = "3", fillOpacity = 0.7, highlight = highlightOptions(weight = 5,
              color = "red", dashArray = "", fillOpacity = 0.7, bringToFront = TRUE),
              label = labels, labelOptions = labelOptions(style = list("font-weight" = "normal", padding = "3px 8px"),
                  textsize = "15px", direction = "auto")) %>%
              addLegend(pal = pal, values = to_neigh$count_listings, opacity = 0.7, title = "No listings", position = "bottomright")  




#     mean income by district
### ==============================================================

pal <- colorBin("viridis",domain=c(min(to_neigh$mean_district_income,na.rm = TRUE),max(to_neigh$mean_district_income,na.rm = TRUE)),
                bins=c(min(to_neigh$mean_district_income),31500,34000,37500,45000,50500,54000,57500,70000,80000,max(to_neigh$mean_district_income)))

labels <- sprintf("<strong>%s</strong>", to_neigh$AREA_NAME) %>% lapply(htmltools::HTML)

to_map <- leaflet(to_neigh) %>% setView(lng = -79.378, lat = 43.695, zoom = 10.5 ) %>% addTiles() %>%
          addMarkers(lng = -79.3841, lat = 43.6534, popup="Toronto City Hall")
to_map  %>% addPolygons(fillColor = ~pal(mean_district_income), weight = 2, opacity = .3, color = "blue",
              dashArray = "3", fillOpacity = 0.7, highlight = highlightOptions(weight = 5,
              color = "red", dashArray = "", fillOpacity = 0.7, bringToFront = TRUE),
              label = labels, labelOptions = labelOptions(style = list("font-weight" = "normal", padding = "3px 8px"),
                  textsize = "15px", direction = "auto")) %>%
              addLegend(pal = pal, values = to_neigh$mean_district_income, opacity = 0.7, title = "Mean income", position = "bottomright")  



#   median 3 brd house like property price by district
### ==============================================================

pal <- colorBin("viridis",domain=c(min(to_neigh$price_3brd_house,na.rm = TRUE),max(to_neigh$price_3brd_house,na.rm = TRUE)),
                bins=round(quantile(to_neigh$price_3brd_house, probs = seq(0,1, length.out = 10),na.rm = TRUE)/10000)*10000)

labels <- sprintf("<strong>%s</strong>", to_neigh$AREA_NAME) %>% lapply(htmltools::HTML)

to_map <- leaflet(to_neigh) %>% setView(lng = -79.378, lat = 43.695, zoom = 10.5 ) %>% addTiles() %>%
          addMarkers(lng = -79.3841, lat = 43.6534, popup="Toronto City Hall")

to_map  %>% addPolygons(fillColor = ~pal(price_3brd_house ), weight = 2, opacity = .3, color = "blue",
              dashArray = "3", fillOpacity = 0.7, highlight = highlightOptions(weight = 5,
              color = "red", dashArray = "", fillOpacity = 0.7, bringToFront = TRUE),
              label = labels, labelOptions = labelOptions(style = list("font-weight" = "normal", padding = "3px 8px"),
                  textsize = "15px", direction = "auto")) %>%
              addLegend(pal = pal, values = to_neigh$price_3brd, opacity = 0.7, title = "Avg. price 3 & 2+1 brd, detached & semi", position = "bottomright")  



#   median 3 brd  condo like property price by district
### ==============================================================


pal <- colorBin("viridis",domain=c(min(to_neigh$price_3brd_condo,na.rm = TRUE),max(to_neigh$price_3brd_condo,na.rm = TRUE)),
                bins=round(quantile(to_neigh$price_3brd_condo, probs = seq(0,1, length.out = 10),na.rm = TRUE)/10000)*10000)

labels <- sprintf("<strong>%s</strong>", to_neigh$AREA_NAME) %>% lapply(htmltools::HTML)

to_map <- leaflet(to_neigh) %>% setView(lng = -79.378, lat = 43.695, zoom = 10.5 ) %>% addTiles() %>%
          addMarkers(lng = -79.3841, lat = 43.6534, popup="Toronto City Hall")

to_map  %>% addPolygons(fillColor = ~pal(price_3brd_condo), weight = 2, opacity = .3, color = "blue",
              dashArray = "3", fillOpacity = 0.7, highlight = highlightOptions(weight = 5,
              color = "red", dashArray = "", fillOpacity = 0.7, bringToFront = TRUE),
              label = labels, labelOptions = labelOptions(style = list("font-weight" = "normal", padding = "3px 8px"),
                  textsize = "15px", direction = "auto")) %>%
              addLegend(pal = pal, values = to_neigh$price_3brd, opacity = 0.7, title = "Avg. price 3 & 2+1 brd, non detached & semi", position = "bottomright")  

```



### 2.4 Verify data quality

????


# 3.  Data preparation
### 3.1 Select data
### 3.2 Clean data
### 3.3 Construct data
### 3.4 Integrate data
### 3.5 Format data



# 4.  Modeling

### 4.1 Select modeling technique & Generate test design
### 4.2 Models

#### 4.2.1. Linear regression
The regression can accommodate both numerical and categorical explanatory variables (the categorical predictors are transformed into dummy variables).

To understand and asses the potential effect of the dimensionality reduction methods  we employed three methods:

a) a linear regression which includes all explanatory variables without any dimensionality reduction method  

b) a stepwise  model for explanatory variable selection based on the Akaike Information Criterion (depends on the model log likelihood and penalizes the addition of parameters)

c) a lasso-ridge penalty approach which, once our likelihood function, which penalizes large coefficients (their square value). As such, only high impact variables, that make it through the penalty, will be retained. To fine tune the weight the penalty function and ensure the result is robust to changes in the  train data, we use 10 folds cross validation.



```{r include=TRUE, message=FALSE, warning=FALSE,echo = FALSE}



# =====================================
# estimate glm function in stats library

GLM_weights<-as.vector(ifelse(trainData$Revenue==TRUE,(1-sum(ifelse(trainData$Revenue==TRUE,1,0))/length(trainData$Revenue))^0.5,
                                                        (sum(ifelse(trainData$Revenue==TRUE,1,0))/length(trainData$Revenue))^0.5  )
                            )

GLM_model_numeric_vars<-c('final_price_log','bedrooms_ag','bedrooms_bg','type','mean_district_income','bathrooms')

GLM_fit_glm_formula=paste0('final_price_log ~ ',
                                '+bedrooms_ag',
                                '+bedrooms_bg',
                                '+type',
                                '+mean_district_income',  
                                '+bathrooms')

GLM_fit_glm <- glm(formula=GLM_fit_glm_formula, data = DF, family = gaussian,x = TRUE, y = TRUE)
#GLM_fit_glm_weighted <- glm(formula=GLM_fit_glm_formula, data = trainData, family = binomial,weights=GLM_weights,x = TRUE, y = TRUE)
#summary(GLM_fit_glm) 



# get model matrix data
MODEL_X_variables<-model.matrix(GLM_fit_glm)[,-1] # -1 removes the intercept
MODEL_Y_variable<-as.vector(DF$final_price_log)




# =====================================
# estimate stepAIC with glm function in MASS & stats libraries

GLM_fit_glm_stepAIC<-stepAIC(GLM_fit_glm,direction='both',trace=0 )
GLM_fit_glm_stepAIC_formula<-formula(GLM_fit_glm_stepAIC)
GLM_fit_glm_stepAIC_glm<- glm(formula=GLM_fit_glm_stepAIC_formula
                    , data = trainData, family = binomial,x = TRUE, y = TRUE)




# =====================================
# estimate for lasso using glmnet function with cross validation for lambda selection from the glmnet library

GLM_fit_glmnet_all_lambda_lasso= glmnet(x=MODEL_X_variables, y=MODEL_Y_variable, family = "gaussian", type.measure = "class", alpha=1)
#plot(GLM_fit_glmnet_all_lambda_lasso)

GLM_fit_glmnet_alllambda_ridge= glmnet(x=MODEL_X_variables, y=MODEL_Y_variable, family = "gaussian", type.measure = "class", alpha=0)
#plot(GLM_fit_glmnet_alllambda_ridge)


#use cross validation for various values of lambda
GLM_fit_glmnet_cvfit = cv.glmnet(x=MODEL_X_variables, y=MODEL_Y_variable, family = "binomial", type.measure = "class",alpha=1)



# =====================================
# outputs for regressions diagnostic and model comparison both for train and test data

GLM_fit_glm_actual_pred<-data.frame(MODEL_Y_variable, fitted.values(GLM_fit_glm))
names(GLM_fit_glm_actual_pred)<-c('final_price_log','Fitted_final_price_log')


GLM_fit_glm_stepAIC_actual_pred<-data.frame(MODEL_Y_variable, fitted.values(GLM_fit_glm_stepAIC_glm))
names(GLM_fit_glm_stepAIC_actual_pred)<-c('final_price_log','Fitted_final_price_log')


GLM_fit_glmnet_cvfit_actual_pred<-data.frame(MODEL_Y_variable, predict(GLM_fit_glmnet_cvfit,newx =MODEL_X_variables , lambda='lambda.1se',type = "response"))
names(GLM_fit_glmnet_cvfit_actual_pred)<-c('final_price_log','Fitted_final_price_log')


```




###### 4.2.1.1  Linear regression - model diagnostics
As the dimensionality reduction will be shown not to have a great effect on the fitted probabilities we report only the results of the third approach, the Logistic Lasso regression with Cross Validation.

```{r include=TRUE, message=FALSE, warning=FALSE,echo = FALSE}
#&nbsp;
#All variables model summary
#tab_model(GLM_fit_glm,show.r2=FALSE,p.style="numeric_stars")
GLM_fit_glm_coeff=coef(summary(GLM_fit_glm))[,c("Estimate","Pr(>|z|)")]
```



```{r include=TRUE, message=FALSE, warning=FALSE,echo = FALSE}
#&nbsp;
#&nbsp;
#Stepwise AIC model summary
#tab_model(GLM_fit_glm_stepAIC_glm,show.r2=FALSE,p.style="numeric_stars")
GLM_fit_glm_stepAIC_coeff=coef(summary(GLM_fit_glm_stepAIC_glm))[,c("Estimate","Pr(>|z|)")]
```



&nbsp;
Model paremeters estimates for Logistic Lasso regression with cross validation tuned lambda (the "lambda.1se" selected variables)
```{r include=TRUE, message=FALSE, warning=FALSE,echo = FALSE}
#=========================================
# we choose the "lambda.1se" coefficients
GLM_fit_glmnet_cvfit_coeff_lambda_min=coef(GLM_fit_glmnet_cvfit, lambda = "lambda.min")
GLM_fit_glmnet_cvfit_coeff=coef(GLM_fit_glmnet_cvfit, lambda = "lambda.1se")
GLM_fit_glmnet_cvfit_coeff_nonzero<-data.frame(
                   features =GLM_fit_glmnet_cvfit_coeff@Dimnames[[1]][which(abs(GLM_fit_glmnet_cvfit_coeff)>0) ],
                    coefs   =GLM_fit_glmnet_cvfit_coeff[ which(abs(GLM_fit_glmnet_cvfit_coeff)>0) ]  #intercept included
)

coef(GLM_fit_glmnet_cvfit , s = "lambda.1se")
#coef(GLM_fit_glmnet_cvfit, s = "lambda.min")
#print(GLM_fit_glmnet_cvfit_coeff_nonzero)


plot(GLM_fit_glmnet_all_lambda_lasso,xvar =c("lambda"),main='Coefficients of model estimated parameters as the Lasso penalty increases',cex.main=0.8)
plot(GLM_fit_glmnet_cvfit,main='Model misclassification error as the lasso penalty increases acorr cross validation samples',cex.main=0.8)

# save model outputs for 
save(GLM_fit_glmnet_cvfit, file="GLM_fit_glmnet_cvfit.Rdata")
```








####  4.2.2. Random Forest Model
Random Forest is an ensembling machine learning algorithm which consists in generating multiple decision trees based on random sampling of the data and the predictor variables and then combining their output. for every explanatory variable a classification is  generated. 
Based on belonging to specific classes of a randomly selected set of variable the decision three is built. The user decides how many variables are used in constructing the decision tree. The construction of the classes for each variables accommodates both categorical and numerical variables (continuous or discontinuous) .

The random sampling serves to de-correlate the trees and subsequently reduce the Variance by averaging them and avoid Overfitting. The user decides how many trees are used for model averaging. 


The random forest model needs to be tuned with respect to the number of decision trees and the number of variables randomly sampled at each stage.

The following two graphs illustrate the impact of the number of trees and the number of variables on the prediction error of the estimated model.

Based on a set of simulations ( not reported) that cover all values between 1 and 15 for the number explanatory variables in a decision tree, we choose 300 as the target number of trees and 10 as the target number of variables. Values larger than these two bring a very small improvement to the performance of teh model.

```{r include=TRUE, message=FALSE, warning=FALSE,echo = FALSE}
# ===========================================
# Random tree Model and parameter tuning

library(randomForest)


# datasets storing results from fine tuning simulations
RANDOMFOREST_fit_as_is_nVar.ALL_misclassified<-data.frame(No_trees=seq_len(500))
RANDOMFOREST_fit_as_is_nVar.FLASE_misclassified<-data.frame(No_trees=seq_len(500))
RANDOMFOREST_fit_as_is_nVar.TRUE_misclassified<-data.frame(No_trees=seq_len(500))


RANDOMFOREST_model_numeric_vars<-c('final_price_log','bedrooms_ag','bedrooms_bg','type','mean_district_income','bathrooms')
for (idx_num_var in seq_len(5)) 
{
    RANDOMFOREST_fit_as_is<-randomForest(final_price_log~.,data=DF[,RANDOMFOREST_model_numeric_vars],type='regression', mtry=idx_num_var,ntree = 500)
    RANDOMFOREST_fit_as_is_nVar.ALL_misclassified[[paste0('No_Vars_',idx_num_var)]]<-as.vector(RANDOMFOREST_fit_as_is$err.rate[,1])
    RANDOMFOREST_fit_as_is_nVar.FLASE_misclassified[[paste0('No_Vars_',idx_num_var)]]<-as.vector(RANDOMFOREST_fit_as_is$err.rate[,2])
    RANDOMFOREST_fit_as_is_nVar.TRUE_misclassified[[paste0('No_Vars_',idx_num_var)]]<-as.vector(RANDOMFOREST_fit_as_is$err.rate[,3])
    
}





# plotting teh results of the fine tuning sumulations
#library(plot3D)
#persp3D(seq_len(500),seq_len(15),as.matrix(RANDOMFOREST_fit_as_is_nVar.TRUE_misclassified[,2:16]),
#       theta=-90, phi=80, axes=TRUE,scale=2, box=TRUE, nticks=5, 
#       ticktype="detailed", xlab="Number of trees", ylab="Number of variables", zlab="Revenue misclassified(%)", 
#       main="Gaussian Kernal with persp3D()")

#xdata=seq_len(500)
#plot(xdata, as.matrix(RANDOMFOREST_fit_as_is_nVar.ALL_misclassified['No_Vars_3']), 
#     type='l',xlab="Number of trees",ylab="Misclasification %",
#     main='Revenue misclassified as % of overall observations by number of trees and number of variables',cex.main = 0.8)
#for (idx_num_var in seq(from = 3, to = 16, by =1) )
#{
#lines(xdata,as.matrix(RANDOMFOREST_fit_as_is_nVar.TRUE_misclassified[,idx_num_var]),col=colours()[idx_num_var])
#}
#legend("topright",names(RANDOMFOREST_fit_as_is_nVar.TRUE_misclassified)[2:16])


RANDOMFOREST_fit_as_is_2_500<-randomForest(RANDOMFOREST_model_text,data=trainData_as_is,type='classification',importance = TRUE, mtry=2,ntree = 500)
RANDOMFOREST_fit_as_is_18_500<-randomForest(RANDOMFOREST_model_text.,data=trainData_as_is,type='classification',importance = TRUE, mtry=5,ntree = 500)

plot(RANDOMFOREST_fit_as_is_2_500,ylim=c(0, 1), main='Model inputs: Number trees=1 to 500, number of variables=2')
legend("topleft", c("Revenue misclassified as % of overall observations","Revenue FALSE misclassified - as % of actual FALSE occurences","Revenu TRUE misclassified as % of actual TRUE occurences"), lty=1, col = c(1, 2,3),bty="n")

plot(RANDOMFOREST_fit_as_is_18_500,ylim=c(0, 1), main='Model inputs: Number trees=1 to 500, number of variables=18')
legend("topleft", c("Revenue misclassified as % of overall observations","Revenue FALSE misclassified - as % of actual FALSE occurences","Revenu TRUE misclassified as % of actual TRUE occurences"), lty=1, col = c(1, 2,3),bty="n")
#varImpPlot(RANDOMFOREST_fit_as_is_18_500)


Target_No_Trees=300
Target_No_Vars=10
RANDOMFOREST_fit_as_is_actual_pred <-data.frame(MODEL_Y_variable, predict(RANDOMFOREST_fit_as_is))
names(RANDOMFOREST_fit_as_is_actual_pred)<-c('Revenue','Fitted_Prob')
RANDOMFOREST_fit_as_is_actual_pred_test_data <-data.frame(MODEL_Y_variable, predict(RANDOMFOREST_fit_as_is,new=testData_as_is))
names(RANDOMFOREST_fit_as_is_actual_pred_test_data)<-c('Revenue','Fitted_Prob')


print(' Variable importance')
RandomForest<-RANDOMFOREST_fit_as_is
varImpPlot(RandomForest)

 
#RANDOMFOREST_fit_glm_inputs_as_is$confusion#RANDOMFOREST_fit_glm_inputs2$votes
```
#####  4.2.1.1 Random Forest Model - model diagnostics






# 5.  Evaluation
### 5.1 Evaluate results
### 5.2 Review process
### 5.3 Determine next steps





# 6.  Deployment
### 6.1 Plan deployment
### 6.2 Plan monitoring and maintenance
### 6.3 Produce final report
### 6.4 Review project


